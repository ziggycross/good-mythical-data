{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Using Scrapetube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scrapetube "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scrapetube\n",
    "from datetime import datetime\n",
    "\n",
    "# Parameters\n",
    "max_entries = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get basic data:\n",
    "\n",
    "For testing, use a low number of max_entries. To remove the limit, use `max_entries = None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMM_url = \"https://www.youtube.com/@GoodMythicalMorning\"\n",
    "video_iterator = scrapetube.get_channel(channel_url=GMM_url, limit=max_entries, sort_by=\"newest\")\n",
    "\n",
    "# New dictionary class with multidimensional get\n",
    "class custom_dict(dict):\n",
    "    def multidim_get(self, *keys):\n",
    "        \"\"\"\n",
    "        Allows .get() method to operate on nested dictionarys.\n",
    "        \"\"\"\n",
    "        value = self\n",
    "        for key in keys:\n",
    "            try:\n",
    "                value = value[key]\n",
    "            except KeyError:\n",
    "                return None\n",
    "        return value\n",
    "\n",
    "# Features to extract from Scrapetube video object\n",
    "\"\"\"\n",
    "#ID\n",
    "    - name\n",
    "    - length\n",
    "    - views\n",
    "    - published date\n",
    "    - thumbnail\n",
    "        - still\n",
    "        - video\n",
    "    - scrape datetime\n",
    "\"\"\"\n",
    "\n",
    "# Function to extract features\n",
    "def get_basic_video_details(video):\n",
    "    video = custom_dict(video)\n",
    "    return {\n",
    "        \"id\": video.multidim_get(\"videoId\"),\n",
    "        \"name\": video.multidim_get(\"title\",\"runs\",0,\"text\"),\n",
    "        \"duration\": video.multidim_get(\"lengthText\",\"simpleText\"),\n",
    "        \"views\": video.multidim_get(\"viewCountText\",\"simpleText\"),\n",
    "        \"published\": video.multidim_get(\"publishedTimeText\",\"simpleText\"),\n",
    "        \"thumbnail\": {\n",
    "            \"still\": video.multidim_get(\"thumbnail\",\"thumbnails\",-1,\"url\"),\n",
    "            \"video\": video.multidim_get(\"richThumbnail\",\"movingThumbnailRenderer\",\"movingThumbnailDetails\",\"thumbnails\",0,\"url\")\n",
    "        },\n",
    "        \"scraped\": datetime.now()\n",
    "    }\n",
    "\n",
    "# Build a dataframe of episodes using our Scrapetube iterator\n",
    "df = pd.DataFrame([\n",
    "    get_basic_video_details(video)\n",
    "    for video\n",
    "    in video_iterator\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean duration values:\n",
    "Standardize format to \"HH:MM:SS\" then convert to integer value (seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _leading_timecode(string, timecode_format=\"00:00:00\"):\n",
    "    \"\"\"\n",
    "    Standardize timecode with leading zeros and delimiters\n",
    "    \"\"\"\n",
    "    return timecode_format[:len(string)-1:-1] + string\n",
    "\n",
    "df[\"duration\"] = np.dot(\n",
    "    df[\"duration\"].apply(_leading_timecode).str.split(\":\", expand=True).astype(int), # Get hours, minutes, and seconds\n",
    "    [3600, 60, 1] # Multiply by 3x1 matrix to convert to total seconds\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean view counts:\n",
    "Convert from format \"##,###,### views\" to integer value (views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"views\"] = df[\"views\"].str.replace(\"\\D\", \"\", regex=True).astype(int, errors=\"ignore\") # Remove any non-integer characters then convert to int"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check output:\n",
    "This is as far as we can get with ScrapeTube. However, we can still get more information about these videos using the YouTube Data API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>duration</th>\n",
       "      <th>views</th>\n",
       "      <th>published</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>scraped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fsJU9mOcvhQ</td>\n",
       "      <td>Our Most Unhinged Moments This Year</td>\n",
       "      <td>1322</td>\n",
       "      <td>493306</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>{'still': 'https://i.ytimg.com/vi/fsJU9mOcvhQ/...</td>\n",
       "      <td>2022-12-22 23:57:51.517487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XiORNYGT-6s</td>\n",
       "      <td>Our Best Food Creations This Year</td>\n",
       "      <td>1331</td>\n",
       "      <td>936835</td>\n",
       "      <td>3 days ago</td>\n",
       "      <td>{'still': 'https://i.ytimg.com/vi/XiORNYGT-6s/...</td>\n",
       "      <td>2022-12-22 23:57:51.517504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B6dXVr0r0Ws</td>\n",
       "      <td>We Tried EVERY Goldfish Flavor</td>\n",
       "      <td>1194</td>\n",
       "      <td>1538449</td>\n",
       "      <td>6 days ago</td>\n",
       "      <td>{'still': 'https://i.ytimg.com/vi/B6dXVr0r0Ws/...</td>\n",
       "      <td>2022-12-22 23:57:51.517511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RPp5CXZVhlc</td>\n",
       "      <td>We Hug For 20 Minutes Straight... For Science</td>\n",
       "      <td>1394</td>\n",
       "      <td>550021</td>\n",
       "      <td>7 days ago</td>\n",
       "      <td>{'still': 'https://i.ytimg.com/vi/RPp5CXZVhlc/...</td>\n",
       "      <td>2022-12-22 23:57:51.517520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JrZP8aAZE9M</td>\n",
       "      <td>Lab Grown Dairy Taste Test</td>\n",
       "      <td>1140</td>\n",
       "      <td>920359</td>\n",
       "      <td>8 days ago</td>\n",
       "      <td>{'still': 'https://i.ytimg.com/vi/JrZP8aAZE9M/...</td>\n",
       "      <td>2022-12-22 23:57:51.517529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                           name  duration  \\\n",
       "0  fsJU9mOcvhQ            Our Most Unhinged Moments This Year      1322   \n",
       "1  XiORNYGT-6s              Our Best Food Creations This Year      1331   \n",
       "2  B6dXVr0r0Ws                 We Tried EVERY Goldfish Flavor      1194   \n",
       "3  RPp5CXZVhlc  We Hug For 20 Minutes Straight... For Science      1394   \n",
       "4  JrZP8aAZE9M                     Lab Grown Dairy Taste Test      1140   \n",
       "\n",
       "     views   published                                          thumbnail  \\\n",
       "0   493306   1 day ago  {'still': 'https://i.ytimg.com/vi/fsJU9mOcvhQ/...   \n",
       "1   936835  3 days ago  {'still': 'https://i.ytimg.com/vi/XiORNYGT-6s/...   \n",
       "2  1538449  6 days ago  {'still': 'https://i.ytimg.com/vi/B6dXVr0r0Ws/...   \n",
       "3   550021  7 days ago  {'still': 'https://i.ytimg.com/vi/RPp5CXZVhlc/...   \n",
       "4   920359  8 days ago  {'still': 'https://i.ytimg.com/vi/JrZP8aAZE9M/...   \n",
       "\n",
       "                     scraped  \n",
       "0 2022-12-22 23:57:51.517487  \n",
       "1 2022-12-22 23:57:51.517504  \n",
       "2 2022-12-22 23:57:51.517511  \n",
       "3 2022-12-22 23:57:51.517520  \n",
       "4 2022-12-22 23:57:51.517529  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"data/gmm-episodes_basic.csv\"\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More data with YouTube Data API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install isodate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import isodate\n",
    "import json\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Parameters\n",
    "basic_dataset = \"data/gmm-episodes_basic.csv\"\n",
    "max_search_pages = 5 # Will not be used if you supply a basic dataset to supply video IDs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Google Cloud project:\n",
    "\n",
    "YouTube Data API calls are limited by a daily quota, we need to run our calls through a Google Cloud project in order to keep track of our quota usage.\n",
    "\n",
    "**Steps:**\n",
    "1. Create a Google Cloud project [here](https://console.cloud.google.com/).\n",
    "2. Enable the [YouTube Data API](https://developers.google.com/youtube/v3) for your project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and import an API Key:\n",
    "\n",
    "To run API calls we will need a Key to link this notebook to our Google Cloud project.\n",
    "\n",
    "**Steps:**\n",
    "1. Create an API Key [here](https://console.cloud.google.com/apis/credentials) for your project.\n",
    "2. Create a file to store this API Key using the `credentials-template.json` template. Name the new file `credentials.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded API key from credentials file.\n"
     ]
    }
   ],
   "source": [
    "credentials_file = \"credentials.json\"\n",
    "\n",
    "with open(credentials_file, \"r\") as fh:\n",
    "    credentials = json.load(fh)\n",
    "\n",
    "assert \"API_Key\" in list(credentials.keys())\n",
    "assert isinstance(credentials[\"API_Key\"], str)\n",
    "\n",
    "print(\"Successfully loaded API key from credentials file.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all videos from channel:\n",
    "\n",
    "We can use the search API to build a list of all videos on the Good Mythical Morning channel, however, we are limited to retrieving 50 videos at a time, so we will need to break this process up into several queries. To do this, we will retrieve one page at a time and then combine our results. \n",
    "\n",
    "For testing, use a low number of pages. To remove this limit, set `max_pages` to an arbitrarily large value (like infinity). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_merge(base, *args):\n",
    "    \"\"\"\n",
    "    Helper function for merging n dictionaries.\n",
    "    \"\"\"\n",
    "    for dictionary in args: base |= dictionary\n",
    "    return base\n",
    "\n",
    "\n",
    "def get_channel_videos_page(channel_id, page_token=None, credential=credentials[\"API_Key\"], max_page_results=50, order=\"date\", parts=[\"id\"]):\n",
    "    \"\"\"\n",
    "    Fucntion to retrieve one page of videos.\n",
    "\n",
    "    Parameters:\n",
    "    channel_id - the ID of the channel to get videos from\n",
    "    page_token - the ID of the page we are looking for. if no page specified this should be 'None'\n",
    "    credential - the API Key used for the query\n",
    "    max_page_results - the number of results to return in each page. this should be limited to 50\n",
    "    order - how to sort the videos we are returning\n",
    "    parts - the pieces of information to retrieve in our query\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"https://www.googleapis.com/youtube/v3/search\" +\\\n",
    "        f\"?key={credential}\" +\\\n",
    "        f\"&channelId={channel_id}\" +\\\n",
    "        f\"&maxResults={max_page_results}\" +\\\n",
    "        f\"&order={order}\" +\\\n",
    "        f\"&part={','.join(parts)}\" +\\\n",
    "        f\"{f'&pageToken={page_token}' if page_token else ''}\"\n",
    "\n",
    "    webpage = requests.get(url)\n",
    "    content = json.loads(webpage.text)\n",
    "\n",
    "    next_page = content.get(\"nextPageToken\")\n",
    "    videos = content.get(\"items\")\n",
    "\n",
    "    if not videos: raise Exception(content[\"error\"][\"message\"])\n",
    "    \n",
    "    videos_data = [\n",
    "        dict_merge(*(video[part] for part in parts))\n",
    "        for video\n",
    "        in videos\n",
    "        if video[\"id\"][\"kind\"] == \"youtube#video\"\n",
    "    ]\n",
    "\n",
    "    return videos_data, next_page\n",
    "\n",
    "\n",
    "def get_channel_videos(channel_id, max_depth=10, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to retrieve all videos from a channel, where count is greater than can be retrieved in a single page.\n",
    "\n",
    "    Parameters:\n",
    "    channel_id - the ID of the channel to get videos from\n",
    "    max_depth - the maximum number of pages to query\n",
    "    **kwargs - arguments to pass to get_channel_videos_page\n",
    "    \"\"\"\n",
    "\n",
    "    all_videos = []\n",
    "    \n",
    "    page_videos, next_page = get_channel_videos_page(channel_id, **kwargs)\n",
    "    all_videos += page_videos\n",
    "\n",
    "    depth = 1\n",
    "    while next_page and depth < max_depth:\n",
    "        page_videos, next_page = get_channel_videos_page(channel_id, page_token=next_page, **kwargs)\n",
    "        all_videos += page_videos\n",
    "        depth += 1\n",
    "\n",
    "    return all_videos\n",
    "\n",
    "if basic_dataset:\n",
    "    video_ids = pd.read_csv(basic_dataset)[\"id\"].to_list()\n",
    "    videos = [{\"videoId\": id, \"kind\": \"youtube#video\"} for id in video_ids]\n",
    "else:\n",
    "    GMM_channel_id = \"UC4PooiX37Pld1T8J5SYT-SQ\"\n",
    "    videos = get_channel_videos(GMM_channel_id, max_depth=max_search_pages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get more data from YouTube DataAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_data(video_id, parts=[\"snippet\", \"statistics\", \"contentDetails\"], credential=credentials[\"API_Key\"]):\n",
    "    \"\"\"\n",
    "    Using the YouTube Data API, get information for a video by it's video ID.\n",
    "    \n",
    "    Parameters:\n",
    "    video_id - the ID of the video to find\n",
    "    parts - the parts of data to return in API call. documentation here https://developers.google.com/youtube/v3/getting-started#partial\n",
    "    credential - the API Key to use for the call\n",
    "    \"\"\"\n",
    "    \n",
    "    url = \"https://www.googleapis.com/youtube/v3/videos\" +\\\n",
    "        f\"?key={credential}\" +\\\n",
    "        f\"&part={','.join(parts)}\" +\\\n",
    "        f\"&id={video_id}\"\n",
    "\n",
    "    webpage = requests.get(url)\n",
    "    content = json.loads(webpage.text)\n",
    "\n",
    "    video = content[\"items\"][0]\n",
    "\n",
    "    return dict_merge(\n",
    "        {\"id\": video_id},\n",
    "        {\"scraped\": pd.to_datetime(str(datetime.utcnow().replace(microsecond=0))+\"+00:00\")},\n",
    "        *(video[part] for part in parts)\n",
    "        )\n",
    "\n",
    "\n",
    "for entry in videos:\n",
    "    entry |= get_video_data(entry[\"videoId\"])\n",
    "\n",
    "df = pd.DataFrame(videos)\n",
    "df = df.drop(columns=[\"kind\", \"videoId\", \"channelId\", \"channelTitle\", \"localized\"]) # Remove redundant columns "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process time formats\n",
    "\n",
    "`publishedAt` and `duration` use a specific format. We can convert the `publishedAt` to date time, and the `duration` to seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"duration\"] = df[\"duration\"].apply(lambda duration: isodate.parse_duration(duration).seconds)\n",
    "df[\"publishedAt\"] = df[\"publishedAt\"].apply(isodate.parse_datetime)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove uninformative columns\n",
    "\n",
    "Some columns have only one unique value, there is no point keeping these, so we can drop them. If you would like to keep them, feel free to skip this block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping column liveBroadcastContent. The single value in column was: 'none'\n",
      "Dropping column favoriteCount. The single value in column was: '0'\n",
      "Dropping column dimension. The single value in column was: '2d'\n",
      "Dropping column projection. The single value in column was: 'rectangular'\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    if df[column].map(str).nunique() == 1:\n",
    "        print(f\"Dropping column {column}. The single value in column was: '{df[column].loc[0]}'\")\n",
    "        df.drop(column,axis=1,inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some columns may not be useful for other reasons, such as having some variation, but not enough to be meaningful. We can drop these too. If you disagree with the reasoning below, feel free to modify the list of columns we've chosen to drop.\n",
    "- `defaultLanguage` and `defaultAudioLanguage`:  \n",
    "    - *All videos are in English. Even though this is encoded in different ways sometimes, all values have the same meaning, so we can drop these features.*\n",
    "- `categoryId`:  \n",
    "    - *All videos since 2014 have been marked as category 24 (\"entertainment\"), therefore, for most purposes, this feature will be unreliable.*\n",
    "- `definition`:\n",
    "    - *Most videos since 2014 have been uploaded in HD, therefore, for most purposes, this feature will be unreliable.*\n",
    "- `caption`:\n",
    "    - *Nearly all videos since 2016 have captions, therefore, for most purposes, this feature will be unreliable.*\n",
    "- `licensedContent`:\n",
    "    - *While a few episodes have `False` values for this feature, there is not nearly enough to be meaningful.*\n",
    "- `regionRestriction`:\n",
    "    - *Only one video has a region restriction (Buddy System Ep. 3, which is blocked in Latvia). We can remove this column, as there isn't enough variation to derive any innsight.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "uninformative_columns = [\"defaultLanguage\", \"defaultAudioLanguage\", \"categoryId\", \"definition\", \"caption\", \"licensedContent\", \"regionRestriction\"]\n",
    "\n",
    "df.drop(columns=uninformative_columns, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform content rating column\n",
    "\n",
    "The content of the values in the `contentRating` column aren't overly useful. To make the data more readable we can convert this feature to a binary one (`ageRestricted`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ageRestricted\"] = df[\"contentRating\"] != {}\n",
    "df.drop(columns=\"contentRating\", inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview data\n",
    "\n",
    "After completed our transformations and dropping features, we can preview our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>scraped</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>thumbnails</th>\n",
       "      <th>tags</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>duration</th>\n",
       "      <th>ageRestricted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fsJU9mOcvhQ</td>\n",
       "      <td>2022-12-23 05:59:54+00:00</td>\n",
       "      <td>2022-12-21 11:00:33+00:00</td>\n",
       "      <td>Our Most Unhinged Moments This Year</td>\n",
       "      <td>Today, we're looking back at our most unhinged...</td>\n",
       "      <td>{'default': {'url': 'https://i.ytimg.com/vi/fs...</td>\n",
       "      <td>[gmm, good mythical morning, rhettandlink, rhe...</td>\n",
       "      <td>493306</td>\n",
       "      <td>19553</td>\n",
       "      <td>742</td>\n",
       "      <td>1322</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XiORNYGT-6s</td>\n",
       "      <td>2022-12-23 05:59:54+00:00</td>\n",
       "      <td>2022-12-19 11:00:15+00:00</td>\n",
       "      <td>Our Best Food Creations This Year</td>\n",
       "      <td>Today, we're looking back at our favorite food...</td>\n",
       "      <td>{'default': {'url': 'https://i.ytimg.com/vi/Xi...</td>\n",
       "      <td>[gmm, good mythical morning, rhettandlink, rhe...</td>\n",
       "      <td>936835</td>\n",
       "      <td>30037</td>\n",
       "      <td>1063</td>\n",
       "      <td>1331</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B6dXVr0r0Ws</td>\n",
       "      <td>2022-12-23 05:59:54+00:00</td>\n",
       "      <td>2022-12-16 11:00:06+00:00</td>\n",
       "      <td>We Tried EVERY Goldfish Flavor</td>\n",
       "      <td>Today, we're eating way too many Goldfish! GMM...</td>\n",
       "      <td>{'default': {'url': 'https://i.ytimg.com/vi/B6...</td>\n",
       "      <td>[gmm, good mythical morning, rhettandlink, rhe...</td>\n",
       "      <td>1538547</td>\n",
       "      <td>50280</td>\n",
       "      <td>1821</td>\n",
       "      <td>1194</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RPp5CXZVhlc</td>\n",
       "      <td>2022-12-23 05:59:54+00:00</td>\n",
       "      <td>2022-12-15 11:00:12+00:00</td>\n",
       "      <td>We Hug For 20 Minutes Straight... For Science</td>\n",
       "      <td>Today, we're joined by Noah Centineo to learn ...</td>\n",
       "      <td>{'default': {'url': 'https://i.ytimg.com/vi/RP...</td>\n",
       "      <td>[gmm, good mythical morning, rhettandlink, rhe...</td>\n",
       "      <td>550021</td>\n",
       "      <td>23600</td>\n",
       "      <td>1405</td>\n",
       "      <td>1394</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JrZP8aAZE9M</td>\n",
       "      <td>2022-12-23 05:59:55+00:00</td>\n",
       "      <td>2022-12-14 11:00:09+00:00</td>\n",
       "      <td>Lab Grown Dairy Taste Test</td>\n",
       "      <td>Today, we're seeing if we can taste the differ...</td>\n",
       "      <td>{'default': {'url': 'https://i.ytimg.com/vi/Jr...</td>\n",
       "      <td>[gmm, good mythical morning, rhettandlink, rhe...</td>\n",
       "      <td>920359</td>\n",
       "      <td>29636</td>\n",
       "      <td>1527</td>\n",
       "      <td>1140</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                   scraped               publishedAt  \\\n",
       "0  fsJU9mOcvhQ 2022-12-23 05:59:54+00:00 2022-12-21 11:00:33+00:00   \n",
       "1  XiORNYGT-6s 2022-12-23 05:59:54+00:00 2022-12-19 11:00:15+00:00   \n",
       "2  B6dXVr0r0Ws 2022-12-23 05:59:54+00:00 2022-12-16 11:00:06+00:00   \n",
       "3  RPp5CXZVhlc 2022-12-23 05:59:54+00:00 2022-12-15 11:00:12+00:00   \n",
       "4  JrZP8aAZE9M 2022-12-23 05:59:55+00:00 2022-12-14 11:00:09+00:00   \n",
       "\n",
       "                                           title  \\\n",
       "0            Our Most Unhinged Moments This Year   \n",
       "1              Our Best Food Creations This Year   \n",
       "2                 We Tried EVERY Goldfish Flavor   \n",
       "3  We Hug For 20 Minutes Straight... For Science   \n",
       "4                     Lab Grown Dairy Taste Test   \n",
       "\n",
       "                                         description  \\\n",
       "0  Today, we're looking back at our most unhinged...   \n",
       "1  Today, we're looking back at our favorite food...   \n",
       "2  Today, we're eating way too many Goldfish! GMM...   \n",
       "3  Today, we're joined by Noah Centineo to learn ...   \n",
       "4  Today, we're seeing if we can taste the differ...   \n",
       "\n",
       "                                          thumbnails  \\\n",
       "0  {'default': {'url': 'https://i.ytimg.com/vi/fs...   \n",
       "1  {'default': {'url': 'https://i.ytimg.com/vi/Xi...   \n",
       "2  {'default': {'url': 'https://i.ytimg.com/vi/B6...   \n",
       "3  {'default': {'url': 'https://i.ytimg.com/vi/RP...   \n",
       "4  {'default': {'url': 'https://i.ytimg.com/vi/Jr...   \n",
       "\n",
       "                                                tags viewCount likeCount  \\\n",
       "0  [gmm, good mythical morning, rhettandlink, rhe...    493306     19553   \n",
       "1  [gmm, good mythical morning, rhettandlink, rhe...    936835     30037   \n",
       "2  [gmm, good mythical morning, rhettandlink, rhe...   1538547     50280   \n",
       "3  [gmm, good mythical morning, rhettandlink, rhe...    550021     23600   \n",
       "4  [gmm, good mythical morning, rhettandlink, rhe...    920359     29636   \n",
       "\n",
       "  commentCount  duration  ageRestricted  \n",
       "0          742      1322          False  \n",
       "1         1063      1331          False  \n",
       "2         1821      1194          False  \n",
       "3         1405      1394          False  \n",
       "4         1527      1140          False  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store large text columns in an archive\n",
    "\n",
    "While we can store our large text objects, like video descriptions directly in our Pandas DataFrame, this gets quite messy when we try to save our data as a CSV. Instead, we will store each description in it's own text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_columns = [\"description\", \"thumbnails\"]\n",
    "\n",
    "zip_path = \"data/gmm_episodes_files.zip\"\n",
    "zip = ZipFile(zip_path, \"w\")\n",
    "\n",
    "\n",
    "def _store_in_archive(archive, name, content, folder=None, extension = \".txt\"):\n",
    "    \"\"\"\n",
    "    Write string to file in an archive\n",
    "    \"\"\"\n",
    "    path = name + extension\n",
    "    if folder: path = folder + \"/\" + path\n",
    "    archive.writestr(path, str(content))\n",
    "\n",
    "\n",
    "def row_to_archive(df, archive, name_column, content_column, **kwargs):\n",
    "    \"\"\"\n",
    "    Store a column of a DataFrame as a folder of files in an archive\n",
    "    \"\"\"\n",
    "    df.apply(lambda row: _store_in_archive(\n",
    "        archive=archive,\n",
    "        name=row[name_column],\n",
    "        content=row[content_column],\n",
    "        folder=content_column,\n",
    "        **kwargs\n",
    "    ), axis=1)\n",
    "\n",
    "\n",
    "for column in archive_columns:\n",
    "    row_to_archive(df, zip, name_column=\"id\", content_column=column)\n",
    "\n",
    "zip.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"data/gmm-episodes_full.csv\"\n",
    "df.drop(columns=archive_columns).to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bece7127777d8681f6b1909d19e009379ddeaa50e8dab6af7ea1715374cc99fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
