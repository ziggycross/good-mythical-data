{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Using Scrapetube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scrapetube "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scrapetube\n",
    "from datetime import datetime\n",
    "\n",
    "# Parameters\n",
    "max_entries = 10\n",
    "basic_data_path = \"data/gmm-episodes_basic.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get basic data:\n",
    "\n",
    "For testing, use a low number of max_entries. To remove the limit, use `max_entries = None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMM_url = \"https://www.youtube.com/@GoodMythicalMorning\"\n",
    "video_iterator = scrapetube.get_channel(channel_url=GMM_url, limit=max_entries, sort_by=\"newest\")\n",
    "\n",
    "# New dictionary class with multidimensional get\n",
    "class custom_dict(dict):\n",
    "    def multidim_get(self, *keys):\n",
    "        \"\"\"\n",
    "        Allows .get() method to operate on nested dictionarys.\n",
    "        \"\"\"\n",
    "        value = self\n",
    "        for key in keys:\n",
    "            try:\n",
    "                value = value[key]\n",
    "            except KeyError:\n",
    "                return None\n",
    "        return value\n",
    "\n",
    "# Features to extract from Scrapetube video object\n",
    "\"\"\"\n",
    "#ID\n",
    "    - name\n",
    "    - length\n",
    "    - views\n",
    "    - published date\n",
    "    - thumbnail\n",
    "        - still\n",
    "        - video\n",
    "    - scrape datetime\n",
    "\"\"\"\n",
    "\n",
    "# Function to extract features\n",
    "def get_basic_video_details(video):\n",
    "    video = custom_dict(video)\n",
    "    return {\n",
    "        \"id\": video.multidim_get(\"videoId\"),\n",
    "        \"name\": video.multidim_get(\"title\",\"runs\",0,\"text\"),\n",
    "        \"duration\": video.multidim_get(\"lengthText\",\"simpleText\"),\n",
    "        \"views\": video.multidim_get(\"viewCountText\",\"simpleText\"),\n",
    "        \"published\": video.multidim_get(\"publishedTimeText\",\"simpleText\"),\n",
    "        \"thumbnail\": {\n",
    "            \"still\": video.multidim_get(\"thumbnail\",\"thumbnails\",-1,\"url\"),\n",
    "            \"video\": video.multidim_get(\"richThumbnail\",\"movingThumbnailRenderer\",\"movingThumbnailDetails\",\"thumbnails\",0,\"url\")\n",
    "        },\n",
    "        \"scraped\": datetime.now()\n",
    "    }\n",
    "\n",
    "# Build a dataframe of episodes using our Scrapetube iterator\n",
    "df = pd.DataFrame([\n",
    "    get_basic_video_details(video)\n",
    "    for video\n",
    "    in video_iterator\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean duration values:\n",
    "Standardize format to \"HH:MM:SS\" then convert to integer value (seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _leading_timecode(string, timecode_format=\"00:00:00\"):\n",
    "    \"\"\"\n",
    "    Standardize timecode with leading zeros and delimiters\n",
    "    \"\"\"\n",
    "    return timecode_format[:len(string)-1:-1] + string\n",
    "\n",
    "df[\"duration\"] = np.dot(\n",
    "    df[\"duration\"].apply(_leading_timecode).str.split(\":\", expand=True).astype(int), # Get hours, minutes, and seconds\n",
    "    [3600, 60, 1] # Multiply by 3x1 matrix to convert to total seconds\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean view counts:\n",
    "Convert from format \"##,###,### views\" to integer value (views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"views\"] = df[\"views\"].str.replace(\"\\D\", \"\", regex=True).astype(int, errors=\"ignore\") # Remove any non-integer characters then convert to int"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check output:\n",
    "This is as far as we can get with ScrapeTube. However, we can still get more information about these videos using the YouTube Data API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>duration</th>\n",
       "      <th>views</th>\n",
       "      <th>published</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>scraped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ti0A16Dvh9M</td>\n",
       "      <td>Best Fancy vs. Fast vs. Frozen Food Marathon</td>\n",
       "      <td>5947</td>\n",
       "      <td>651620</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>{'still': 'https://i.ytimg.com/vi/ti0A16Dvh9M/...</td>\n",
       "      <td>2023-01-01 22:36:46.580632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KfNEM2ofBuU</td>\n",
       "      <td>Best GMM Guests Marathon</td>\n",
       "      <td>6152</td>\n",
       "      <td>371808</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>{'still': 'https://i.ytimg.com/vi/KfNEM2ofBuU/...</td>\n",
       "      <td>2023-01-01 22:36:46.580652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fk05OGKe85k</td>\n",
       "      <td>Best International Taste Tests Marathon</td>\n",
       "      <td>6175</td>\n",
       "      <td>1018533</td>\n",
       "      <td>6 days ago</td>\n",
       "      <td>{'still': 'https://i.ytimg.com/vi/fk05OGKe85k/...</td>\n",
       "      <td>2023-01-01 22:36:46.580659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4mhWk1M1mV8</td>\n",
       "      <td>Our Craziest Food Competitions This Year</td>\n",
       "      <td>1242</td>\n",
       "      <td>583193</td>\n",
       "      <td>9 days ago</td>\n",
       "      <td>{'still': 'https://i.ytimg.com/vi/4mhWk1M1mV8/...</td>\n",
       "      <td>2023-01-01 22:36:46.580665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fsJU9mOcvhQ</td>\n",
       "      <td>Our Most Unhinged Moments This Year</td>\n",
       "      <td>1322</td>\n",
       "      <td>642530</td>\n",
       "      <td>11 days ago</td>\n",
       "      <td>{'still': 'https://i.ytimg.com/vi/fsJU9mOcvhQ/...</td>\n",
       "      <td>2023-01-01 22:36:46.580671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                          name  duration  \\\n",
       "0  ti0A16Dvh9M  Best Fancy vs. Fast vs. Frozen Food Marathon      5947   \n",
       "1  KfNEM2ofBuU                      Best GMM Guests Marathon      6152   \n",
       "2  fk05OGKe85k       Best International Taste Tests Marathon      6175   \n",
       "3  4mhWk1M1mV8      Our Craziest Food Competitions This Year      1242   \n",
       "4  fsJU9mOcvhQ           Our Most Unhinged Moments This Year      1322   \n",
       "\n",
       "     views    published                                          thumbnail  \\\n",
       "0   651620   2 days ago  {'still': 'https://i.ytimg.com/vi/ti0A16Dvh9M/...   \n",
       "1   371808   4 days ago  {'still': 'https://i.ytimg.com/vi/KfNEM2ofBuU/...   \n",
       "2  1018533   6 days ago  {'still': 'https://i.ytimg.com/vi/fk05OGKe85k/...   \n",
       "3   583193   9 days ago  {'still': 'https://i.ytimg.com/vi/4mhWk1M1mV8/...   \n",
       "4   642530  11 days ago  {'still': 'https://i.ytimg.com/vi/fsJU9mOcvhQ/...   \n",
       "\n",
       "                     scraped  \n",
       "0 2023-01-01 22:36:46.580632  \n",
       "1 2023-01-01 22:36:46.580652  \n",
       "2 2023-01-01 22:36:46.580659  \n",
       "3 2023-01-01 22:36:46.580665  \n",
       "4 2023-01-01 22:36:46.580671  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(basic_data_path, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More data with YouTube Data API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install isodate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import isodate\n",
    "import json\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Parameters\n",
    "full_data_path = \"data/gmm-episodes_basic.csv\"\n",
    "zip_path = \"data/gmm_episodes_files.zip\"\n",
    "max_search_pages = 5 # Will not be used if you supply a basic dataset to supply video IDs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Google Cloud project:\n",
    "\n",
    "YouTube Data API calls are limited by a daily quota, we need to run our calls through a Google Cloud project in order to keep track of our quota usage.\n",
    "\n",
    "**Steps:**\n",
    "1. Create a Google Cloud project [here](https://console.cloud.google.com/).\n",
    "2. Enable the [YouTube Data API](https://developers.google.com/youtube/v3) for your project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and import an API Key:\n",
    "\n",
    "To run API calls we will need a Key to link this notebook to our Google Cloud project.\n",
    "\n",
    "**Steps:**\n",
    "1. Create an API Key [here](https://console.cloud.google.com/apis/credentials) for your project.\n",
    "2. Create a file to store this API Key using the `credentials-template.json` template. Name the new file `credentials.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded API key from credentials file.\n"
     ]
    }
   ],
   "source": [
    "credentials_file = \"credentials.json\"\n",
    "\n",
    "with open(credentials_file, \"r\") as fh:\n",
    "    credentials = json.load(fh)\n",
    "\n",
    "assert \"API_Key\" in list(credentials.keys())\n",
    "assert isinstance(credentials[\"API_Key\"], str)\n",
    "\n",
    "print(\"Successfully loaded API key from credentials file.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all videos from channel:\n",
    "\n",
    "We can use the search API to build a list of all videos on the Good Mythical Morning channel, however, we are limited to retrieving 50 videos at a time, so we will need to break this process up into several queries. To do this, we will retrieve one page at a time and then combine our results. \n",
    "\n",
    "For testing, use a low number of pages. To remove this limit, set `max_pages` to an arbitrarily large value (like infinity). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_merge(base, *args):\n",
    "    \"\"\"\n",
    "    Helper function for merging n dictionaries.\n",
    "    \"\"\"\n",
    "    for dictionary in args: base |= dictionary\n",
    "    return base\n",
    "\n",
    "\n",
    "def get_channel_videos_page(channel_id, page_token=None, credential=credentials[\"API_Key\"], max_page_results=50, order=\"date\", parts=[\"id\"]):\n",
    "    \"\"\"\n",
    "    Fucntion to retrieve one page of videos.\n",
    "\n",
    "    Parameters:\n",
    "    channel_id - the ID of the channel to get videos from\n",
    "    page_token - the ID of the page we are looking for. if no page specified this should be 'None'\n",
    "    credential - the API Key used for the query\n",
    "    max_page_results - the number of results to return in each page. this should be limited to 50\n",
    "    order - how to sort the videos we are returning\n",
    "    parts - the pieces of information to retrieve in our query\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"https://www.googleapis.com/youtube/v3/search\" +\\\n",
    "        f\"?key={credential}\" +\\\n",
    "        f\"&channelId={channel_id}\" +\\\n",
    "        f\"&maxResults={max_page_results}\" +\\\n",
    "        f\"&order={order}\" +\\\n",
    "        f\"&part={','.join(parts)}\" +\\\n",
    "        f\"{f'&pageToken={page_token}' if page_token else ''}\"\n",
    "\n",
    "    webpage = requests.get(url)\n",
    "    content = json.loads(webpage.text)\n",
    "\n",
    "    next_page = content.get(\"nextPageToken\")\n",
    "    videos = content.get(\"items\")\n",
    "\n",
    "    if not videos: raise Exception(content[\"error\"][\"message\"])\n",
    "    \n",
    "    videos_data = [\n",
    "        dict_merge(*(video[part] for part in parts))\n",
    "        for video\n",
    "        in videos\n",
    "        if video[\"id\"][\"kind\"] == \"youtube#video\"\n",
    "    ]\n",
    "\n",
    "    return videos_data, next_page\n",
    "\n",
    "\n",
    "def get_channel_videos(channel_id, max_depth=10, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to retrieve all videos from a channel, where count is greater than can be retrieved in a single page.\n",
    "\n",
    "    Parameters:\n",
    "    channel_id - the ID of the channel to get videos from\n",
    "    max_depth - the maximum number of pages to query\n",
    "    **kwargs - arguments to pass to get_channel_videos_page\n",
    "    \"\"\"\n",
    "\n",
    "    all_videos = []\n",
    "    \n",
    "    page_videos, next_page = get_channel_videos_page(channel_id, **kwargs)\n",
    "    all_videos += page_videos\n",
    "\n",
    "    depth = 1\n",
    "    while next_page and depth < max_depth:\n",
    "        page_videos, next_page = get_channel_videos_page(channel_id, page_token=next_page, **kwargs)\n",
    "        all_videos += page_videos\n",
    "        depth += 1\n",
    "\n",
    "    return all_videos\n",
    "\n",
    "if basic_data_path:\n",
    "    video_ids = pd.read_csv(basic_data_path)[\"id\"].to_list()\n",
    "    videos = [{\"videoId\": id, \"kind\": \"youtube#video\"} for id in video_ids]\n",
    "else:\n",
    "    GMM_channel_id = \"UC4PooiX37Pld1T8J5SYT-SQ\"\n",
    "    videos = get_channel_videos(GMM_channel_id, max_depth=max_search_pages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from YouTube DataAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_data(video_id, parts=[\"snippet\", \"statistics\", \"contentDetails\"], credential=credentials[\"API_Key\"]):\n",
    "    \"\"\"\n",
    "    Using the YouTube Data API, get information for a video by it's video ID.\n",
    "    \n",
    "    Parameters:\n",
    "    video_id - the ID of the video to find\n",
    "    parts - the parts of data to return in API call. documentation here https://developers.google.com/youtube/v3/getting-started#partial\n",
    "    credential - the API Key to use for the call\n",
    "    \"\"\"\n",
    "    \n",
    "    url = \"https://www.googleapis.com/youtube/v3/videos\" +\\\n",
    "        f\"?key={credential}\" +\\\n",
    "        f\"&part={','.join(parts)}\" +\\\n",
    "        f\"&id={video_id}\"\n",
    "\n",
    "    webpage = requests.get(url)\n",
    "    content = json.loads(webpage.text)\n",
    "\n",
    "    video = content[\"items\"][0]\n",
    "\n",
    "    return dict_merge(\n",
    "        {\"id\": video_id},\n",
    "        {\"scraped\": pd.to_datetime(str(datetime.utcnow().replace(microsecond=0))+\"+00:00\")},\n",
    "        *(video[part] for part in parts)\n",
    "        )\n",
    "\n",
    "\n",
    "for entry in videos:\n",
    "    entry |= get_video_data(entry[\"videoId\"])\n",
    "\n",
    "df = pd.DataFrame(videos)\n",
    "df = df.drop(columns=[\"kind\", \"videoId\", \"channelId\", \"channelTitle\", \"localized\"]) # Remove redundant columns "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process time formats\n",
    "\n",
    "`publishedAt` and `duration` use a specific format. We can convert the `publishedAt` to date time, and the `duration` to seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"duration\"] = df[\"duration\"].apply(lambda duration: isodate.parse_duration(duration).seconds)\n",
    "df[\"publishedAt\"] = df[\"publishedAt\"].apply(isodate.parse_datetime)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove constant columns\n",
    "\n",
    "Some columns have only one unique value, there is no point keeping these, so we can drop them. If you would like to keep them, feel free to skip this block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping column tags. The single value in column was: '['gmm', 'good mythical morning', 'rhettandlink', 'rhett and link', 'mythical', 'rhett', 'mclaughlin', 'link', 'neal', 'will it', 'taste test']'\n",
      "Dropping column categoryId. The single value in column was: '24'\n",
      "Dropping column liveBroadcastContent. The single value in column was: 'none'\n",
      "Dropping column defaultLanguage. The single value in column was: 'en'\n",
      "Dropping column defaultAudioLanguage. The single value in column was: 'en'\n",
      "Dropping column favoriteCount. The single value in column was: '0'\n",
      "Dropping column dimension. The single value in column was: '2d'\n",
      "Dropping column definition. The single value in column was: 'hd'\n",
      "Dropping column caption. The single value in column was: 'true'\n",
      "Dropping column licensedContent. The single value in column was: 'True'\n",
      "Dropping column contentRating. The single value in column was: '{}'\n",
      "Dropping column projection. The single value in column was: 'rectangular'\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    if df[column].map(str).nunique() == 1:\n",
    "        print(f\"Dropping column {column}. The single value in column was: '{df[column].loc[0]}'\")\n",
    "        df.drop(column,axis=1,inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove uninformative columns\n",
    "\n",
    "Some columns may not be useful for other reasons, such as having some variation, but not enough to be meaningful. We can drop these too. If you disagree with the reasoning below, feel free to modify the list of columns we've chosen to drop.\n",
    "- `defaultLanguage` and `defaultAudioLanguage`:  \n",
    "    - *All videos are in English. Even though this is encoded in different ways sometimes, all values have the same meaning, so we can drop these features.*\n",
    "- `categoryId`:  \n",
    "    - *All videos since 2014 have been marked as category 24 (\"entertainment\"), therefore, for most purposes, this feature will be unreliable.*\n",
    "- `definition`:\n",
    "    - *Most videos since 2014 have been uploaded in HD, therefore, for most purposes, this feature will be unreliable.*\n",
    "- `caption`:\n",
    "    - *Nearly all videos since 2016 have captions, therefore, for most purposes, this feature will be unreliable.*\n",
    "- `licensedContent`:\n",
    "    - *While a few episodes have `False` values for this feature, there is not nearly enough to be meaningful.*\n",
    "- `regionRestriction`:\n",
    "    - *Only one video has a region restriction (Buddy System Ep. 3, which is blocked in Latvia). We can remove this column, as there isn't enough variation to derive any innsight.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "uninformative_columns = [\"defaultLanguage\", \"defaultAudioLanguage\", \"categoryId\", \"definition\", \"caption\", \"licensedContent\", \"regionRestriction\"]\n",
    "\n",
    "df.drop(columns=[column for column in uninformative_columns if column in df.columns], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform content rating column\n",
    "\n",
    "The content of the values in the `contentRating` column aren't overly useful. To make the data more readable we can convert this feature to a binary one (`ageRestricted`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"contentRating\" in df.columns:\n",
    "    df[\"ageRestricted\"] = df[\"contentRating\"] != {}\n",
    "    df.drop(columns=\"contentRating\", inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview data\n",
    "\n",
    "After completed our transformations and dropping features, we can preview our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>scraped</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>thumbnails</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ti0A16Dvh9M</td>\n",
       "      <td>2023-01-02 03:36:47+00:00</td>\n",
       "      <td>2022-12-30 11:00:27+00:00</td>\n",
       "      <td>Best Fancy vs. Fast vs. Frozen Food Marathon</td>\n",
       "      <td>Today, we're having FANCY VS FAST VS FROZEN FO...</td>\n",
       "      <td>{'default': {'url': 'https://i.ytimg.com/vi/ti...</td>\n",
       "      <td>651620</td>\n",
       "      <td>13899</td>\n",
       "      <td>671</td>\n",
       "      <td>5947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KfNEM2ofBuU</td>\n",
       "      <td>2023-01-02 03:36:47+00:00</td>\n",
       "      <td>2022-12-28 11:00:18+00:00</td>\n",
       "      <td>Best GMM Guests Marathon</td>\n",
       "      <td>Today, we're looking back at some of the BEST ...</td>\n",
       "      <td>{'default': {'url': 'https://i.ytimg.com/vi/Kf...</td>\n",
       "      <td>371808</td>\n",
       "      <td>10354</td>\n",
       "      <td>723</td>\n",
       "      <td>6152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fk05OGKe85k</td>\n",
       "      <td>2023-01-02 03:36:47+00:00</td>\n",
       "      <td>2022-12-26 11:00:19+00:00</td>\n",
       "      <td>Best International Taste Tests Marathon</td>\n",
       "      <td>Today, we're having an INTERNATIONAL TASTE TES...</td>\n",
       "      <td>{'default': {'url': 'https://i.ytimg.com/vi/fk...</td>\n",
       "      <td>1018533</td>\n",
       "      <td>22415</td>\n",
       "      <td>1180</td>\n",
       "      <td>6175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4mhWk1M1mV8</td>\n",
       "      <td>2023-01-02 03:36:47+00:00</td>\n",
       "      <td>2022-12-23 11:00:02+00:00</td>\n",
       "      <td>Our Craziest Food Competitions This Year</td>\n",
       "      <td>Today, we're looking back at some of our CRAZI...</td>\n",
       "      <td>{'default': {'url': 'https://i.ytimg.com/vi/4m...</td>\n",
       "      <td>583193</td>\n",
       "      <td>18143</td>\n",
       "      <td>622</td>\n",
       "      <td>1242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fsJU9mOcvhQ</td>\n",
       "      <td>2023-01-02 03:36:48+00:00</td>\n",
       "      <td>2022-12-21 11:00:33+00:00</td>\n",
       "      <td>Our Most Unhinged Moments This Year</td>\n",
       "      <td>Today, we're looking back at our most unhinged...</td>\n",
       "      <td>{'default': {'url': 'https://i.ytimg.com/vi/fs...</td>\n",
       "      <td>642530</td>\n",
       "      <td>23667</td>\n",
       "      <td>790</td>\n",
       "      <td>1322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                   scraped               publishedAt  \\\n",
       "0  ti0A16Dvh9M 2023-01-02 03:36:47+00:00 2022-12-30 11:00:27+00:00   \n",
       "1  KfNEM2ofBuU 2023-01-02 03:36:47+00:00 2022-12-28 11:00:18+00:00   \n",
       "2  fk05OGKe85k 2023-01-02 03:36:47+00:00 2022-12-26 11:00:19+00:00   \n",
       "3  4mhWk1M1mV8 2023-01-02 03:36:47+00:00 2022-12-23 11:00:02+00:00   \n",
       "4  fsJU9mOcvhQ 2023-01-02 03:36:48+00:00 2022-12-21 11:00:33+00:00   \n",
       "\n",
       "                                          title  \\\n",
       "0  Best Fancy vs. Fast vs. Frozen Food Marathon   \n",
       "1                      Best GMM Guests Marathon   \n",
       "2       Best International Taste Tests Marathon   \n",
       "3      Our Craziest Food Competitions This Year   \n",
       "4           Our Most Unhinged Moments This Year   \n",
       "\n",
       "                                         description  \\\n",
       "0  Today, we're having FANCY VS FAST VS FROZEN FO...   \n",
       "1  Today, we're looking back at some of the BEST ...   \n",
       "2  Today, we're having an INTERNATIONAL TASTE TES...   \n",
       "3  Today, we're looking back at some of our CRAZI...   \n",
       "4  Today, we're looking back at our most unhinged...   \n",
       "\n",
       "                                          thumbnails viewCount likeCount  \\\n",
       "0  {'default': {'url': 'https://i.ytimg.com/vi/ti...    651620     13899   \n",
       "1  {'default': {'url': 'https://i.ytimg.com/vi/Kf...    371808     10354   \n",
       "2  {'default': {'url': 'https://i.ytimg.com/vi/fk...   1018533     22415   \n",
       "3  {'default': {'url': 'https://i.ytimg.com/vi/4m...    583193     18143   \n",
       "4  {'default': {'url': 'https://i.ytimg.com/vi/fs...    642530     23667   \n",
       "\n",
       "  commentCount  duration  \n",
       "0          671      5947  \n",
       "1          723      6152  \n",
       "2         1180      6175  \n",
       "3          622      1242  \n",
       "4          790      1322  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store large text columns in an archive\n",
    "\n",
    "While we can store our large text objects, like video descriptions directly in our Pandas DataFrame, this gets quite messy when we try to save our data as a CSV. Instead, we will store each description in it's own text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_columns = [\"description\", \"thumbnails\"]\n",
    "\n",
    "zip = ZipFile(zip_path, \"w\")\n",
    "\n",
    "\n",
    "def _store_string_in_archive(archive, name, content, folder=None, extension = \".txt\"):\n",
    "    \"\"\"\n",
    "    Write string to file in an archive\n",
    "    \"\"\"\n",
    "    path = name + extension\n",
    "    if folder: path = folder + \"/\" + path\n",
    "    archive.writestr(path, str(content))\n",
    "\n",
    "\n",
    "def row_to_archive(df, archive, name_column, content_column, function=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Store a column of a DataFrame as a folder of files in an archive\n",
    "    \"\"\"\n",
    "    df.apply(lambda row: _store_string_in_archive(\n",
    "        archive=archive,\n",
    "        name=row[name_column],\n",
    "        content=function(row[content_column]) if function else row[content_column],\n",
    "        folder=content_column,\n",
    "        **kwargs\n",
    "    ), axis=1)\n",
    "\n",
    "\n",
    "for column in archive_columns:\n",
    "    row_to_archive(df, zip, name_column=\"id\", content_column=column)\n",
    "\n",
    "zip.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=archive_columns).to_csv(full_data_path, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download video captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install srt\n",
    "# !pip install youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import srt\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api.formatters import SRTFormatter\n",
    "\n",
    "# Parameters\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new row for video transcripts\n",
    "\n",
    "We can collect video transcripts from YouTube videos using the `youtube=transcript-api` library. This library uses the YouTube Web Client API, which does not use our Google Cloud project quota.\n",
    "\n",
    "This segment is quite slow - expect it to take a significant amount of time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"transcript\"] = df[\"id\"].apply(lambda id: YouTubeTranscriptApi.get_transcript(id))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the transcripts to .srt files\n",
    "\n",
    "We can use the SRT Formatter built into the transcript library to save our transcripts as `.srt` files in our archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatter = SRTFormatter()\n",
    "\n",
    "zip = ZipFile(zip_path, \"a\")\n",
    "\n",
    "row_to_archive(df, zip, name_column=\"id\", content_column=\"transcript\", function=formatter.format_transcript, extension=\".srt\")\n",
    "\n",
    "zip.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize transcript formatting in pandas\n",
    "\n",
    "Currently, transcripts are stored as a list of dictionaries. This is fine, but if we want to load our dataset from a file, our subtitles will be in a different format (as defined by our srt reading library) - we should convert to this format now, just so all future processing works in the target format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def srt_to_subtitle_object(srt_string):\n",
    "    return list(srt.parse(srt_string))\n",
    "\n",
    "def youtube_transcript_to_subtitle_object(youtube_transcript_object):\n",
    "    return srt_to_subtitle_object(formatter.format_transcript(youtube_transcript_object))\n",
    "\n",
    "df[\"transcript\"] = df[\"transcript\"].apply(youtube_transcript_to_subtitle_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>scraped</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>thumbnails</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>duration</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ti0A16Dvh9M</td>\n",
       "      <td>2023-01-02 03:36:47+00:00</td>\n",
       "      <td>2022-12-30 11:00:27+00:00</td>\n",
       "      <td>Best Fancy vs. Fast vs. Frozen Food Marathon</td>\n",
       "      <td>Today, we're having FANCY VS FAST VS FROZEN FO...</td>\n",
       "      <td>{'default': {'url': 'https://i.ytimg.com/vi/ti...</td>\n",
       "      <td>651620</td>\n",
       "      <td>13899</td>\n",
       "      <td>671</td>\n",
       "      <td>5947</td>\n",
       "      <td>[Subtitle(index=1, start=datetime.timedelta(mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KfNEM2ofBuU</td>\n",
       "      <td>2023-01-02 03:36:47+00:00</td>\n",
       "      <td>2022-12-28 11:00:18+00:00</td>\n",
       "      <td>Best GMM Guests Marathon</td>\n",
       "      <td>Today, we're looking back at some of the BEST ...</td>\n",
       "      <td>{'default': {'url': 'https://i.ytimg.com/vi/Kf...</td>\n",
       "      <td>371808</td>\n",
       "      <td>10354</td>\n",
       "      <td>723</td>\n",
       "      <td>6152</td>\n",
       "      <td>[Subtitle(index=1, start=datetime.timedelta(mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fk05OGKe85k</td>\n",
       "      <td>2023-01-02 03:36:47+00:00</td>\n",
       "      <td>2022-12-26 11:00:19+00:00</td>\n",
       "      <td>Best International Taste Tests Marathon</td>\n",
       "      <td>Today, we're having an INTERNATIONAL TASTE TES...</td>\n",
       "      <td>{'default': {'url': 'https://i.ytimg.com/vi/fk...</td>\n",
       "      <td>1018533</td>\n",
       "      <td>22415</td>\n",
       "      <td>1180</td>\n",
       "      <td>6175</td>\n",
       "      <td>[Subtitle(index=1, start=datetime.timedelta(mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4mhWk1M1mV8</td>\n",
       "      <td>2023-01-02 03:36:47+00:00</td>\n",
       "      <td>2022-12-23 11:00:02+00:00</td>\n",
       "      <td>Our Craziest Food Competitions This Year</td>\n",
       "      <td>Today, we're looking back at some of our CRAZI...</td>\n",
       "      <td>{'default': {'url': 'https://i.ytimg.com/vi/4m...</td>\n",
       "      <td>583193</td>\n",
       "      <td>18143</td>\n",
       "      <td>622</td>\n",
       "      <td>1242</td>\n",
       "      <td>[Subtitle(index=1, start=datetime.timedelta(mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fsJU9mOcvhQ</td>\n",
       "      <td>2023-01-02 03:36:48+00:00</td>\n",
       "      <td>2022-12-21 11:00:33+00:00</td>\n",
       "      <td>Our Most Unhinged Moments This Year</td>\n",
       "      <td>Today, we're looking back at our most unhinged...</td>\n",
       "      <td>{'default': {'url': 'https://i.ytimg.com/vi/fs...</td>\n",
       "      <td>642530</td>\n",
       "      <td>23667</td>\n",
       "      <td>790</td>\n",
       "      <td>1322</td>\n",
       "      <td>[Subtitle(index=1, start=datetime.timedelta(mi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                   scraped               publishedAt  \\\n",
       "0  ti0A16Dvh9M 2023-01-02 03:36:47+00:00 2022-12-30 11:00:27+00:00   \n",
       "1  KfNEM2ofBuU 2023-01-02 03:36:47+00:00 2022-12-28 11:00:18+00:00   \n",
       "2  fk05OGKe85k 2023-01-02 03:36:47+00:00 2022-12-26 11:00:19+00:00   \n",
       "3  4mhWk1M1mV8 2023-01-02 03:36:47+00:00 2022-12-23 11:00:02+00:00   \n",
       "4  fsJU9mOcvhQ 2023-01-02 03:36:48+00:00 2022-12-21 11:00:33+00:00   \n",
       "\n",
       "                                          title  \\\n",
       "0  Best Fancy vs. Fast vs. Frozen Food Marathon   \n",
       "1                      Best GMM Guests Marathon   \n",
       "2       Best International Taste Tests Marathon   \n",
       "3      Our Craziest Food Competitions This Year   \n",
       "4           Our Most Unhinged Moments This Year   \n",
       "\n",
       "                                         description  \\\n",
       "0  Today, we're having FANCY VS FAST VS FROZEN FO...   \n",
       "1  Today, we're looking back at some of the BEST ...   \n",
       "2  Today, we're having an INTERNATIONAL TASTE TES...   \n",
       "3  Today, we're looking back at some of our CRAZI...   \n",
       "4  Today, we're looking back at our most unhinged...   \n",
       "\n",
       "                                          thumbnails viewCount likeCount  \\\n",
       "0  {'default': {'url': 'https://i.ytimg.com/vi/ti...    651620     13899   \n",
       "1  {'default': {'url': 'https://i.ytimg.com/vi/Kf...    371808     10354   \n",
       "2  {'default': {'url': 'https://i.ytimg.com/vi/fk...   1018533     22415   \n",
       "3  {'default': {'url': 'https://i.ytimg.com/vi/4m...    583193     18143   \n",
       "4  {'default': {'url': 'https://i.ytimg.com/vi/fs...    642530     23667   \n",
       "\n",
       "  commentCount  duration                                         transcript  \n",
       "0          671      5947  [Subtitle(index=1, start=datetime.timedelta(mi...  \n",
       "1          723      6152  [Subtitle(index=1, start=datetime.timedelta(mi...  \n",
       "2         1180      6175  [Subtitle(index=1, start=datetime.timedelta(mi...  \n",
       "3          622      1242  [Subtitle(index=1, start=datetime.timedelta(mi...  \n",
       "4          790      1322  [Subtitle(index=1, start=datetime.timedelta(mi...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bece7127777d8681f6b1909d19e009379ddeaa50e8dab6af7ea1715374cc99fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
