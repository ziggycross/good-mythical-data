{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Using Scrapetube:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get basic data:\n",
    "\n",
    "For testing, use a low number of max_entries. To remove the limit, use `max_entries = None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scrapetube "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scrapetube\n",
    "\n",
    "# Parameters\n",
    "max_entries = 50\n",
    "\n",
    "GMM_url = \"https://www.youtube.com/@GoodMythicalMorning\"\n",
    "video_iterator = scrapetube.get_channel(channel_url=GMM_url, limit=max_entries, sort_by=\"newest\")\n",
    "\n",
    "# New dictionary class with multidimensional get\n",
    "class custom_dict(dict):\n",
    "    def multidim_get(self, *keys):\n",
    "        \"\"\"\n",
    "        Allows .get() method to operate on nested dictionarys.\n",
    "        \"\"\"\n",
    "        value = self\n",
    "        for key in keys:\n",
    "            try:\n",
    "                value = value[key]\n",
    "            except KeyError:\n",
    "                return None\n",
    "        return value\n",
    "\n",
    "# Features to extract from Scrapetube video object\n",
    "\"\"\"\n",
    "#ID\n",
    "    - name\n",
    "    - length\n",
    "    - views\n",
    "    - published date\n",
    "    - thumbnail\n",
    "        - still\n",
    "        - video\n",
    "    - scrape datetime\n",
    "\"\"\"\n",
    "\n",
    "# Function to extract features\n",
    "def get_basic_video_details(video):\n",
    "    video = custom_dict(video)\n",
    "    return {\n",
    "        \"id\": video.multidim_get(\"videoId\"),\n",
    "        \"name\": video.multidim_get(\"title\",\"runs\",0,\"text\"),\n",
    "        \"duration\": video.multidim_get(\"lengthText\",\"simpleText\"),\n",
    "        \"views\": video.multidim_get(\"viewCountText\",\"simpleText\"),\n",
    "        \"published\": video.multidim_get(\"publishedTimeText\",\"simpleText\"),\n",
    "        \"thumbnail\": {\n",
    "            \"still\": video.multidim_get(\"thumbnail\",\"thumbnails\",-1,\"url\"),\n",
    "            \"video\": video.multidim_get(\"richThumbnail\",\"movingThumbnailRenderer\",\"movingThumbnailDetails\",\"thumbnails\",0,\"url\")\n",
    "        },\n",
    "        \"scraped\": datetime.now()\n",
    "    }\n",
    "\n",
    "# Build a dataframe of episodes using our Scrapetube iterator\n",
    "df = pd.DataFrame([\n",
    "    get_basic_video_details(video)\n",
    "    for video\n",
    "    in video_iterator\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean duration values:\n",
    "Standardize format to \"HH:MM:SS\" then convert to integer value (seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _leading_timecode(string, timecode_format=\"00:00:00\"):\n",
    "    \"\"\"\n",
    "    Standardize timecode with leading zeros and delimiters\n",
    "    \"\"\"\n",
    "    return timecode_format[:len(string)-1:-1] + string\n",
    "\n",
    "df[\"duration\"] = np.dot(\n",
    "    df[\"duration\"].apply(_leading_timecode).str.split(\":\", expand=True).astype(int), # Get hours, minutes, and seconds\n",
    "    [3600, 60, 1] # Multiply by 3x1 matrix to convert to total seconds\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean view counts:\n",
    "Convert from format \"##,###,### views\" to integer value (views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"views\"] = df[\"views\"].str.replace(\"\\D\", \"\", regex=True).astype(int, errors=\"ignore\") # Remove any non-integer characters then convert to int"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check output:\n",
    "This is as far as we can get with ScrapeTube. However, we can still get more information about these videos using the YouTube Data API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>duration</th>\n",
       "      <th>views</th>\n",
       "      <th>published</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>scraped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fsJU9mOcvhQ</td>\n",
       "      <td>Our Most Unhinged Moments This Year</td>\n",
       "      <td>1322</td>\n",
       "      <td>440681</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>{'still': 'https://i.ytimg.com/vi/fsJU9mOcvhQ/...</td>\n",
       "      <td>2022-12-22 12:42:15.635369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XiORNYGT-6s</td>\n",
       "      <td>Our Best Food Creations This Year</td>\n",
       "      <td>1331</td>\n",
       "      <td>915049</td>\n",
       "      <td>3 days ago</td>\n",
       "      <td>{'still': 'https://i.ytimg.com/vi/XiORNYGT-6s/...</td>\n",
       "      <td>2022-12-22 12:42:15.635387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B6dXVr0r0Ws</td>\n",
       "      <td>We Tried EVERY Goldfish Flavor</td>\n",
       "      <td>1194</td>\n",
       "      <td>1503539</td>\n",
       "      <td>6 days ago</td>\n",
       "      <td>{'still': 'https://i.ytimg.com/vi/B6dXVr0r0Ws/...</td>\n",
       "      <td>2022-12-22 12:42:15.635396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RPp5CXZVhlc</td>\n",
       "      <td>We Hug For 20 Minutes Straight... For Science</td>\n",
       "      <td>1394</td>\n",
       "      <td>545424</td>\n",
       "      <td>7 days ago</td>\n",
       "      <td>{'still': 'https://i.ytimg.com/vi/RPp5CXZVhlc/...</td>\n",
       "      <td>2022-12-22 12:42:15.635403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JrZP8aAZE9M</td>\n",
       "      <td>Lab Grown Dairy Taste Test</td>\n",
       "      <td>1140</td>\n",
       "      <td>914341</td>\n",
       "      <td>8 days ago</td>\n",
       "      <td>{'still': 'https://i.ytimg.com/vi/JrZP8aAZE9M/...</td>\n",
       "      <td>2022-12-22 12:42:15.635410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                           name  duration  \\\n",
       "0  fsJU9mOcvhQ            Our Most Unhinged Moments This Year      1322   \n",
       "1  XiORNYGT-6s              Our Best Food Creations This Year      1331   \n",
       "2  B6dXVr0r0Ws                 We Tried EVERY Goldfish Flavor      1194   \n",
       "3  RPp5CXZVhlc  We Hug For 20 Minutes Straight... For Science      1394   \n",
       "4  JrZP8aAZE9M                     Lab Grown Dairy Taste Test      1140   \n",
       "\n",
       "     views   published                                          thumbnail  \\\n",
       "0   440681   1 day ago  {'still': 'https://i.ytimg.com/vi/fsJU9mOcvhQ/...   \n",
       "1   915049  3 days ago  {'still': 'https://i.ytimg.com/vi/XiORNYGT-6s/...   \n",
       "2  1503539  6 days ago  {'still': 'https://i.ytimg.com/vi/B6dXVr0r0Ws/...   \n",
       "3   545424  7 days ago  {'still': 'https://i.ytimg.com/vi/RPp5CXZVhlc/...   \n",
       "4   914341  8 days ago  {'still': 'https://i.ytimg.com/vi/JrZP8aAZE9M/...   \n",
       "\n",
       "                     scraped  \n",
       "0 2022-12-22 12:42:15.635369  \n",
       "1 2022-12-22 12:42:15.635387  \n",
       "2 2022-12-22 12:42:15.635396  \n",
       "3 2022-12-22 12:42:15.635403  \n",
       "4 2022-12-22 12:42:15.635410  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"data/gmm-episodes_basic.csv\"\n",
    "df.to_csv(output_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More data with YouTube Data API:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Google Cloud project:\n",
    "\n",
    "YouTube Data API calls are limited by a daily quota, we need to run our calls through a Google Cloud project in order to keep track of our quota usage.\n",
    "\n",
    "**Steps:**\n",
    "1. Create a Google Cloud project [here](https://console.cloud.google.com/).\n",
    "2. Enable the [YouTube Data API](https://developers.google.com/youtube/v3) for your project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and import an API Key:\n",
    "\n",
    "To run API calls we will need a Key to link this notebook to our Google Cloud project.\n",
    "\n",
    "**Steps:**\n",
    "1. Create an API Key [here](https://console.cloud.google.com/apis/credentials) for your project.\n",
    "2. Create a file to store this API Key using the `credentials-template.json` template. Name the new file `credentials.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded API key from credentials file.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "credentials_file = \"credentials.json\"\n",
    "\n",
    "with open(credentials_file, \"r\") as fh:\n",
    "    credentials = json.load(fh)\n",
    "\n",
    "assert \"API_Key\" in list(credentials.keys())\n",
    "assert isinstance(credentials[\"API_Key\"], str)\n",
    "\n",
    "print(\"Successfully loaded API key from credentials file.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all videos from channel:\n",
    "\n",
    "We can use the search API to build a list of all videos on the Good Mythical Morning channel, however, we are limited to retrieving 50 videos at a time, so we will need to break this process up into several queries. To do this, we will retrieve one page at a time and then combine our results. \n",
    "\n",
    "For testing, use a low number of pages. To remove this limit, set `max_pages` to an arbitrarily large value (like infinity). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['kind', 'videoId'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "max_pages = 1\n",
    "results_per_page = 50\n",
    "\n",
    "\n",
    "def dict_merge(base, *args):\n",
    "    \"\"\"\n",
    "    Helper function for merging n dictionaries.\n",
    "    \"\"\"\n",
    "    for dictionary in args: base |= dictionary\n",
    "    return base\n",
    "\n",
    "\n",
    "def get_channel_videos_page(channel_id, page_token=None, credential=credentials[\"API_Key\"], max_page_results=50, order=\"date\", parts=[\"id\"]):\n",
    "    \"\"\"\n",
    "    Fucntion to retrieve one page of videos.\n",
    "\n",
    "    Parameters:\n",
    "    channel_id - the ID of the channel to get videos from\n",
    "    page_token - the ID of the page we are looking for. if no page specified this should be 'None'\n",
    "    credential - the API Key used for the query\n",
    "    max_page_results - the number of results to return in each page. this should be limited to 50\n",
    "    order - how to sort the videos we are returning\n",
    "    parts - the pieces of information to retrieve in our query\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"https://www.googleapis.com/youtube/v3/search\" +\\\n",
    "        f\"?key={credential}\" +\\\n",
    "        f\"&channelId={channel_id}\" +\\\n",
    "        f\"&maxResults={max_page_results}\" +\\\n",
    "        f\"&order={order}\" +\\\n",
    "        f\"&part={','.join(parts)}\" +\\\n",
    "        f\"{f'&pageToken={page_token}' if page_token else ''}\"\n",
    "\n",
    "    webpage = requests.get(url)\n",
    "    content = json.loads(webpage.text)\n",
    "\n",
    "    next_page = content.get(\"nextPageToken\")\n",
    "    videos = content.get(\"items\")\n",
    "\n",
    "    if not videos: raise Exception(content[\"error\"][\"message\"])\n",
    "    \n",
    "    videos_data = [\n",
    "        dict_merge(*(video[part] for part in parts))\n",
    "        for video\n",
    "        in videos\n",
    "        if video[\"id\"][\"kind\"] == \"youtube#video\"\n",
    "    ]\n",
    "\n",
    "    return videos_data, next_page\n",
    "\n",
    "\n",
    "def get_channel_videos(channel_id, max_depth=10, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to retrieve all videos from a channel, where count is greater than can be retrieved in a single page.\n",
    "\n",
    "    Parameters:\n",
    "    channel_id - the ID of the channel to get videos from\n",
    "    max_depth - the maximum number of pages to query\n",
    "    **kwargs - arguments to pass to get_channel_videos_page\n",
    "    \"\"\"\n",
    "\n",
    "    all_videos = []\n",
    "    \n",
    "    page_videos, next_page = get_channel_videos_page(channel_id, **kwargs)\n",
    "    all_videos += page_videos\n",
    "\n",
    "    depth = 1\n",
    "    while next_page and depth < max_depth:\n",
    "        page_videos, next_page = get_channel_videos_page(channel_id, page_token=next_page, **kwargs)\n",
    "        all_videos += page_videos\n",
    "        depth += 1\n",
    "\n",
    "    return all_videos\n",
    "\n",
    "\n",
    "GMM_channel_id = \"UC4PooiX37Pld1T8J5SYT-SQ\"\n",
    "videos = get_channel_videos(GMM_channel_id, max_depth=max_pages, max_page_results=results_per_page)\n",
    "videos[0].keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get more data from YouTube DataAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>scraped</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>thumbnails</th>\n",
       "      <th>tags</th>\n",
       "      <th>categoryId</th>\n",
       "      <th>liveBroadcastContent</th>\n",
       "      <th>defaultLanguage</th>\n",
       "      <th>...</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>duration</th>\n",
       "      <th>dimension</th>\n",
       "      <th>definition</th>\n",
       "      <th>caption</th>\n",
       "      <th>licensedContent</th>\n",
       "      <th>contentRating</th>\n",
       "      <th>projection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fsJU9mOcvhQ</td>\n",
       "      <td>2022-12-22 18:42:18+00:00</td>\n",
       "      <td>2022-12-21T11:00:33Z</td>\n",
       "      <td>Our Most Unhinged Moments This Year</td>\n",
       "      <td>Today, we're looking back at our most unhinged...</td>\n",
       "      <td>{'default': {'url': 'https://i.ytimg.com/vi/fs...</td>\n",
       "      <td>[gmm, good mythical morning, rhettandlink, rhe...</td>\n",
       "      <td>24</td>\n",
       "      <td>none</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td>18314</td>\n",
       "      <td>0</td>\n",
       "      <td>709</td>\n",
       "      <td>PT22M2S</td>\n",
       "      <td>2d</td>\n",
       "      <td>hd</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>{}</td>\n",
       "      <td>rectangular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86b9eToglMY</td>\n",
       "      <td>2022-12-22 18:42:18+00:00</td>\n",
       "      <td>2022-12-20T17:00:08Z</td>\n",
       "      <td>Link's Too Sensitive To Eat Ice Cream</td>\n",
       "      <td>They're SENSITIVE! #shorts\\n\\nRemember this #G...</td>\n",
       "      <td>{'default': {'url': 'https://i.ytimg.com/vi/86...</td>\n",
       "      <td>[gmm, good mythical morning, rhettandlink, rhe...</td>\n",
       "      <td>24</td>\n",
       "      <td>none</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td>10065</td>\n",
       "      <td>0</td>\n",
       "      <td>129</td>\n",
       "      <td>PT37S</td>\n",
       "      <td>2d</td>\n",
       "      <td>hd</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>{}</td>\n",
       "      <td>rectangular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XiORNYGT-6s</td>\n",
       "      <td>2022-12-22 18:42:18+00:00</td>\n",
       "      <td>2022-12-19T11:00:15Z</td>\n",
       "      <td>Our Best Food Creations This Year</td>\n",
       "      <td>Today, we're looking back at our favorite food...</td>\n",
       "      <td>{'default': {'url': 'https://i.ytimg.com/vi/Xi...</td>\n",
       "      <td>[gmm, good mythical morning, rhettandlink, rhe...</td>\n",
       "      <td>24</td>\n",
       "      <td>none</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td>29635</td>\n",
       "      <td>0</td>\n",
       "      <td>1065</td>\n",
       "      <td>PT22M11S</td>\n",
       "      <td>2d</td>\n",
       "      <td>hd</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>{}</td>\n",
       "      <td>rectangular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GDWWseS9phk</td>\n",
       "      <td>2022-12-22 18:42:19+00:00</td>\n",
       "      <td>2022-12-17T11:00:13Z</td>\n",
       "      <td>Bologna Is 500 Years Old #ad UberOne</td>\n",
       "      <td>This is an ad for Uber One, the membership for...</td>\n",
       "      <td>{'default': {'url': 'https://i.ytimg.com/vi/GD...</td>\n",
       "      <td>[gmm, good mythical morning, rhettandlink, rhe...</td>\n",
       "      <td>24</td>\n",
       "      <td>none</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td>3413</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>PT59S</td>\n",
       "      <td>2d</td>\n",
       "      <td>hd</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>{}</td>\n",
       "      <td>rectangular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B6dXVr0r0Ws</td>\n",
       "      <td>2022-12-22 18:42:19+00:00</td>\n",
       "      <td>2022-12-16T11:00:06Z</td>\n",
       "      <td>We Tried EVERY Goldfish Flavor</td>\n",
       "      <td>Today, we're eating way too many Goldfish! GMM...</td>\n",
       "      <td>{'default': {'url': 'https://i.ytimg.com/vi/B6...</td>\n",
       "      <td>[gmm, good mythical morning, rhettandlink, rhe...</td>\n",
       "      <td>24</td>\n",
       "      <td>none</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td>49532</td>\n",
       "      <td>0</td>\n",
       "      <td>1807</td>\n",
       "      <td>PT19M54S</td>\n",
       "      <td>2d</td>\n",
       "      <td>hd</td>\n",
       "      <td>true</td>\n",
       "      <td>True</td>\n",
       "      <td>{}</td>\n",
       "      <td>rectangular</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                   scraped           publishedAt  \\\n",
       "0  fsJU9mOcvhQ 2022-12-22 18:42:18+00:00  2022-12-21T11:00:33Z   \n",
       "1  86b9eToglMY 2022-12-22 18:42:18+00:00  2022-12-20T17:00:08Z   \n",
       "2  XiORNYGT-6s 2022-12-22 18:42:18+00:00  2022-12-19T11:00:15Z   \n",
       "3  GDWWseS9phk 2022-12-22 18:42:19+00:00  2022-12-17T11:00:13Z   \n",
       "4  B6dXVr0r0Ws 2022-12-22 18:42:19+00:00  2022-12-16T11:00:06Z   \n",
       "\n",
       "                                   title  \\\n",
       "0    Our Most Unhinged Moments This Year   \n",
       "1  Link's Too Sensitive To Eat Ice Cream   \n",
       "2      Our Best Food Creations This Year   \n",
       "3   Bologna Is 500 Years Old #ad UberOne   \n",
       "4         We Tried EVERY Goldfish Flavor   \n",
       "\n",
       "                                         description  \\\n",
       "0  Today, we're looking back at our most unhinged...   \n",
       "1  They're SENSITIVE! #shorts\\n\\nRemember this #G...   \n",
       "2  Today, we're looking back at our favorite food...   \n",
       "3  This is an ad for Uber One, the membership for...   \n",
       "4  Today, we're eating way too many Goldfish! GMM...   \n",
       "\n",
       "                                          thumbnails  \\\n",
       "0  {'default': {'url': 'https://i.ytimg.com/vi/fs...   \n",
       "1  {'default': {'url': 'https://i.ytimg.com/vi/86...   \n",
       "2  {'default': {'url': 'https://i.ytimg.com/vi/Xi...   \n",
       "3  {'default': {'url': 'https://i.ytimg.com/vi/GD...   \n",
       "4  {'default': {'url': 'https://i.ytimg.com/vi/B6...   \n",
       "\n",
       "                                                tags categoryId  \\\n",
       "0  [gmm, good mythical morning, rhettandlink, rhe...         24   \n",
       "1  [gmm, good mythical morning, rhettandlink, rhe...         24   \n",
       "2  [gmm, good mythical morning, rhettandlink, rhe...         24   \n",
       "3  [gmm, good mythical morning, rhettandlink, rhe...         24   \n",
       "4  [gmm, good mythical morning, rhettandlink, rhe...         24   \n",
       "\n",
       "  liveBroadcastContent defaultLanguage  ... likeCount favoriteCount  \\\n",
       "0                 none              en  ...     18314             0   \n",
       "1                 none              en  ...     10065             0   \n",
       "2                 none              en  ...     29635             0   \n",
       "3                 none              en  ...      3413             0   \n",
       "4                 none              en  ...     49532             0   \n",
       "\n",
       "  commentCount  duration dimension definition caption licensedContent  \\\n",
       "0          709   PT22M2S        2d         hd    true            True   \n",
       "1          129     PT37S        2d         hd    true            True   \n",
       "2         1065  PT22M11S        2d         hd    true            True   \n",
       "3           41     PT59S        2d         hd    true            True   \n",
       "4         1807  PT19M54S        2d         hd    true            True   \n",
       "\n",
       "  contentRating   projection  \n",
       "0            {}  rectangular  \n",
       "1            {}  rectangular  \n",
       "2            {}  rectangular  \n",
       "3            {}  rectangular  \n",
       "4            {}  rectangular  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_video_data(video_id, parts=[\"snippet\", \"statistics\", \"contentDetails\"], credential=credentials[\"API_Key\"]):\n",
    "    \"\"\"\n",
    "    Using the YouTube Data API, get information for a video by it's video ID.\n",
    "    \n",
    "    Parameters:\n",
    "    video_id - the ID of the video to find\n",
    "    parts - the parts of data to return in API call. documentation here https://developers.google.com/youtube/v3/getting-started#partial\n",
    "    credential - the API Key to use for the call\n",
    "    \"\"\"\n",
    "    \n",
    "    url = \"https://www.googleapis.com/youtube/v3/videos\" +\\\n",
    "        f\"?key={credential}\" +\\\n",
    "        f\"&part={','.join(parts)}\" +\\\n",
    "        f\"&id={video_id}\"\n",
    "\n",
    "    webpage = requests.get(url)\n",
    "    content = json.loads(webpage.text)\n",
    "\n",
    "    video = content[\"items\"][0]\n",
    "\n",
    "    return dict_merge(\n",
    "        {\"id\": video_id},\n",
    "        {\"scraped\": pd.to_datetime(str(datetime.utcnow().replace(microsecond=0))+\"+00:00\")},\n",
    "        *(video[part] for part in parts)\n",
    "        )\n",
    "\n",
    "\n",
    "for entry in videos:\n",
    "    entry |= get_video_data(entry[\"videoId\"])\n",
    "\n",
    "df = pd.DataFrame(videos)\n",
    "df = df.drop(columns=[\"kind\", \"videoId\", \"channelId\", \"channelTitle\", \"localized\"]) # Remove redundant columns \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process time formats\n",
    "\n",
    "`publishedAt` and `duration` use a specific format. We can convert the `publishedAt` to date time, and the `duration` to seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import isodate\n",
    "\n",
    "df[\"duration\"] = df[\"duration\"].apply(lambda duration: isodate.parse_duration(duration).seconds)\n",
    "df[\"publishedAt\"] = df[\"publishedAt\"].apply(isodate.parse_datetime)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove uninformative columns\n",
    "\n",
    "Some columns have only one unique value, there is no point keeping these, so we can drop them. If you would like to keep them, feel free to skip this block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping column categoryId. The single value in column was: '24'\n",
      "Dropping column liveBroadcastContent. The single value in column was: 'none'\n",
      "Dropping column defaultLanguage. The single value in column was: 'en'\n",
      "Dropping column defaultAudioLanguage. The single value in column was: 'en'\n",
      "Dropping column favoriteCount. The single value in column was: '0'\n",
      "Dropping column dimension. The single value in column was: '2d'\n",
      "Dropping column definition. The single value in column was: 'hd'\n",
      "Dropping column licensedContent. The single value in column was: 'True'\n",
      "Dropping column contentRating. The single value in column was: '{}'\n",
      "Dropping column projection. The single value in column was: 'rectangular'\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    if df[column].map(str).nunique() == 1:\n",
    "        print(f\"Dropping column {column}. The single value in column was: '{df[column].loc[0]}'\")\n",
    "        df.drop(column,axis=1,inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store large text columns in an archive\n",
    "\n",
    "While we can store our large text objects, like video descriptions directly in our Pandas DataFrame, this gets quite messy when we try to save our data as a CSV. Instead, we will store each description in it's own text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "archive_columns = [\"description\"]\n",
    "\n",
    "zip_path = \"data/gmm_episodes_files.zip\"\n",
    "zip = ZipFile(zip_path, \"w\")\n",
    "\n",
    "\n",
    "def _store_in_archive(archive, name, content, folder=None, extension = \".txt\"):\n",
    "    \"\"\"\n",
    "    Write string to file in an archive\n",
    "    \"\"\"\n",
    "    path = name + extension\n",
    "    if folder: path = folder + \"/\" + path\n",
    "    archive.writestr(path, str(content))\n",
    "\n",
    "\n",
    "def row_to_archive(df, archive, name_column, content_column, **kwargs):\n",
    "    \"\"\"\n",
    "    Store a column of a DataFrame as a folder of files in an archive\n",
    "    \"\"\"\n",
    "    df.apply(lambda row: _store_in_archive(\n",
    "        archive=archive,\n",
    "        name=row[name_column],\n",
    "        content=row[content_column],\n",
    "        folder=content_column,\n",
    "        **kwargs\n",
    "    ), axis=1)\n",
    "\n",
    "\n",
    "for column in archive_columns:\n",
    "    row_to_archive(df, zip, name_column=\"id\", content_column=column)\n",
    "\n",
    "zip.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"data/gmm-episodes_full.csv\"\n",
    "df.drop(columns=archive_columns).to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bece7127777d8681f6b1909d19e009379ddeaa50e8dab6af7ea1715374cc99fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
